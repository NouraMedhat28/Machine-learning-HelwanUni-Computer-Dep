{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "989b76b2cb6cb90eef00ceb72b900b48b068d6d1"
   },
   "source": [
    "# Statistics Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ed83f411c19a07c0c8a48b8d978828ea3589707"
   },
   "source": [
    "Hi!\n",
    "\n",
    "This notebook is a gentle tutorial to essential concepts in statistics. I try to present the concepts in a fun and interactive way and I encourage you to play with the code to get a better grasp of the concepts.\n",
    "\n",
    "I will be using a \"[Toy Dataset](https://www.kaggle.com/carlolepelaars/toy-dataset)\" to illustrate concepts in this kernel.\n",
    "\n",
    "The Jupyter Notebook and dataset are also available as a [Github repository](https://github.com/CarloLepelaars/stats_tutorial).\n",
    "\n",
    "![](https://i.stack.imgur.com/c88K3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e44e6d3b18b6da66ae052463a2e8a1afc9bf008"
   },
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "564fcdf42264765f9e9cb0021393ff225da36356"
   },
   "source": [
    "- [Preparation](#1)\n",
    "- [Discrete and Continuous Variables](#2)\n",
    "  - PMF (Probability Mass Function)\n",
    "  - PDF (Probability Density Function)\n",
    "  - CDF (Cumulative Distribution Function)\n",
    "- [Distributions](#3)\n",
    "  - Uniform Distribution\n",
    "  - Normal Distribution\n",
    "  - Binomial Distribution\n",
    "  - Poisson Distribution\n",
    "  - Log-normal Distribution\n",
    "- [Summary Statistics and Moments](#4)\n",
    "- [Bias, MSE and SE](#5)\n",
    "- [Sampling Methods](#6)\n",
    "- [Covariance](#7)\n",
    "- [Correlation](#8)\n",
    "- [Linear Regression](#9)\n",
    "  - Anscombe's Quartet\n",
    "- [Bootstrapping](#10)\n",
    "- [Hypothesis Testing](#11)\n",
    "  - p-value\n",
    "  - q-q plot\n",
    "- [Outliers](#12)\n",
    "  - Grubbs Test\n",
    "  - Tukey's Method\n",
    "- [Overfitting](#20)\n",
    "  - Prevention of Overfitting\n",
    "  - Cross-Validation\n",
    "- [Generalized Linear Models (GLMs)](#13)\n",
    "  - Link Functions\n",
    "  - Logistic Regression\n",
    "- [Frequentist vs. Bayes](#14)\n",
    "- [Bonus: Free Statistics Courses](#15)\n",
    "- [Sources](#16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "282586fdc29e40b96c386c63a254232f9fffdffb"
   },
   "source": [
    "## Preparation <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "f45ba6a511f519d35d488b71d3d8ab3189c0b175"
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "# Standard Dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "\n",
    "# Visualization\n",
    "from pylab import *\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistics\n",
    "from statistics import median\n",
    "from scipy import signal\n",
    "from scipy.misc import factorial\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import sem, binom, lognorm, poisson, bernoulli, spearmanr\n",
    "from scipy.fftpack import fft, fftshift\n",
    "\n",
    "# Scikit-learn for Machine Learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Seed for reproducability\n",
    "seed = 12345\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Kaggle Directory for Kernels\n",
    "KAGGLE_DIR = '../input/'\n",
    "\n",
    "# Read in csv of Toy Dataset\n",
    "# We will use this dataset throughout the tutorial\n",
    "df = pd.read_csv(KAGGLE_DIR + 'toy_dataset.csv')\n",
    "\n",
    "# Files and file sizes\n",
    "print('\\n# Files and file sizes')\n",
    "for file in os.listdir(KAGGLE_DIR):\n",
    "    print('{}| {} MB'.format(file.ljust(30), \n",
    "                             str(round(os.path.getsize(KAGGLE_DIR + file) / 1000000, 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13eff7a79e90263f0f7760f8448a633d07b1253f"
   },
   "source": [
    "## Discrete and Continuous Variables <a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "48cd9a66e56c4614b368f6daa775f2def096081a"
   },
   "source": [
    "A discrete variable is a variable that can only take on a \"countable\" number of values. If you can count a set of items, then it’s a discrete variable. An example of a discrete variable is the outcome of a dice. It can only have 1 of 6 different possible outcomes and is therefore discrete. A discrete random variable can have an infinite number of values. For example, the whole set of natural numbers (1,2,3,etc.) is countable and therefore discrete. \n",
    "\n",
    "A continuous variable takes on an \"uncountable\" number of values. An example of a continuous variable is length. Length can be measured to an arbitrary degree and is therefore continuous.\n",
    "\n",
    "In statistics we represent a distribution of discrete variables with PMF's (Probability Mass Functions) and CDF's (Cumulative Distribution Functions). We represent distributions of continuous variables with PDF's (Probability Density Functions) and CDF's. \n",
    "\n",
    "The PMF defines the probability of all possible values x of the random variable. A PDF is the same but for continuous values.\n",
    "The CDF represents the probability that the random variable X will have an outcome less or equal to the value x. The name CDF is used for both discrete and continuous distributions.\n",
    "\n",
    "The functions that describe PMF's, PDF's and CDF's can be quite daunting at first, but their visual counterpart often looks quite intuitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2624a2129e53c2395974c97fad41014c2badaf72"
   },
   "source": [
    "### PMF (Probability Mass Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b8f282bd4f0c2f3f309eb918f931c0a6a1e0fa47"
   },
   "source": [
    "Here we visualize a PMF of a binomial distribution. You can see that the possible values are all integers. For example, no values are between 50 and 51. \n",
    "\n",
    "The PMF of a binomial distribution in function form:\n",
    "\n",
    "![](http://reliabilityace.com/formulas/binomial-pmf.png)\n",
    "\n",
    "See the \"[Distributions](#3)\" sections for more information on binomial distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "db0e9869de1bd1698d82aef683f1e728332cca42"
   },
   "outputs": [],
   "source": [
    "# PMF Visualization\n",
    "n = 100\n",
    "p = 0.5\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(17,5))\n",
    "x = np.arange(binom.ppf(0.01, n, p), binom.ppf(0.99, n, p))\n",
    "ax.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='Binomial PMF')\n",
    "ax.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)\n",
    "rv = binom(n, p)\n",
    "#ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1, label='frozen PMF')\n",
    "ax.legend(loc='best', frameon=False, fontsize='xx-large')\n",
    "plt.title('PMF of a binomial distribution (n=100, p=0.5)', fontsize='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "296d295ab15c5ee1459ded68e057cfd5643e38b1"
   },
   "source": [
    "### PDF (Probability Density Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fad05c69f87d0e8d15ce668c3afaf59544043b9d"
   },
   "source": [
    "The PDF is the same as a PMF, but continuous. It can be said that the distribution has an infinite number of possible values. Here we visualize a simple normal distribution with a mean of 0 and standard deviation of 1.\n",
    "\n",
    "PDF of a normal distribution in formula form:\n",
    "\n",
    "![](https://www.mhnederlof.nl/images/normalpdf.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "d1269471579a6259bd4e30420f3930589000108f"
   },
   "outputs": [],
   "source": [
    "# Plot normal distribution\n",
    "mu = 0\n",
    "variance = 1\n",
    "sigma = sqrt(variance)\n",
    "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.plot(x, stats.norm.pdf(x, mu, sigma), label='Normal Distribution')\n",
    "plt.title('Normal Distribution with mean = 0 and std = 1')\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fdc08067fcf98556c39d06352faec7fa18653c08"
   },
   "source": [
    "### CDF (Cumulative Distribution Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe673d32cc307926fd6612c469a14092555534ce"
   },
   "source": [
    "The CDF maps the probability that a random variable X will take a value of less than or equal to a value x (P(X ≤  x)). CDF's can be discrete or continuous. In this section we visualize the continuous case. You can see in the plot that the CDF accumulates all probabilities and is therefore bounded between 0 ≤ x ≤ 1.\n",
    "\n",
    "The CDF of a normal distribution as a formula:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/187f33664b79492eedf4406c66d67f9fe5f524ea)\n",
    "\n",
    "*Note: erf means \"[error function](https://en.wikipedia.org/wiki/Error_function)\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "50ac54008528325e5d5a84a010edc1b87cb3b204"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "X  = np.arange(-2, 2, 0.01)\n",
    "Y  = exp(-X ** 2)\n",
    "\n",
    "# Normalize data\n",
    "Y = Y / (0.01 * Y).sum()\n",
    "\n",
    "# Plot the PDF and CDF\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title('Continuous Normal Distributions', fontsize='xx-large')\n",
    "plot(X, Y, label='Probability Density Function (PDF)')\n",
    "plot(X, np.cumsum(Y * 0.01), 'r', label='Cumulative Distribution Function (CDF)')\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e620ad8bcf498e33bedcaba57975a93a9891e29"
   },
   "source": [
    "## Distributions <a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d35d79ca82a18094a76df6bacf43ade19e1c983"
   },
   "source": [
    "A Probability distribution tells us something about the likelihood of each value of the random variable.\n",
    "\n",
    "A random variable X is a function that maps events to real numbers.\n",
    "\n",
    "The visualizations in this section are of discrete distributions. Many of these distributions can however also be continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "93ab51edf08b7e337396b0d73d8dc2c138a85362"
   },
   "source": [
    "### Uniform Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ba34f6480af6f231252954e1e7fe5180ba52550"
   },
   "source": [
    "A Uniform distribution is pretty straightforward. Every value has an equal change of occuring. Therefore, the distribution consists of random values with no patterns in them. In this example we generate random floating numbers between 0 and 1.\n",
    "\n",
    "The PDF of a Uniform Distribution:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/648692e002b720347c6c981aeec2a8cca7f4182f)\n",
    "\n",
    "CDF:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/eeeeb233753cfe775b24e3fec2f371ee8cdc63a6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a19749b9b84493a9840e3bc4fe66336a4910f53c"
   },
   "outputs": [],
   "source": [
    "# Uniform distribution (between 0 and 1)\n",
    "uniform_dist = np.random.random(1000)\n",
    "uniform_df = pd.DataFrame({'value' : uniform_dist})\n",
    "uniform_dist = pd.Series(uniform_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "d681fa3cc5c5458542b005b58e9b219bf2628882"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "sns.scatterplot(data=uniform_df)\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.title('Scatterplot of a Random/Uniform Distribution', fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a4c9ac5d4332f800a15c9fed1de603c3b64a5162"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "sns.distplot(uniform_df)\n",
    "plt.title('Random/Uniform distribution', fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d80f0ce34730da4b8bc6fec9747a82e6d7273a83"
   },
   "source": [
    "### Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d64629b0d2c90c90ebe16841a04fccf6a4a4387a"
   },
   "source": [
    "A normal distribution (also called Gaussian or Bell Curve) is very common and convenient. This is mainly because of the [Central Limit Theorem (CLT)](https://en.wikipedia.org/wiki/Central_limit_theorem), which states that as the amount independent random samples (like multiple coin flips) goes to infinity the distribution of the sample mean tends towards a normal distribution.\n",
    "\n",
    "PDF of a normal distribution:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/2ce7e315b02666699e0cd8ea5fb1a3e0c287cd9d)\n",
    "\n",
    "CDF:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/187f33664b79492eedf4406c66d67f9fe5f524ea)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "f7b7e4657705105148671b00662874b7eb2c98ef"
   },
   "outputs": [],
   "source": [
    "# Generate Normal Distribution\n",
    "normal_dist = np.random.randn(10000)\n",
    "normal_df = pd.DataFrame({'value' : normal_dist})\n",
    "# Create a Pandas Series for easy sample function\n",
    "normal_dist = pd.Series(normal_dist)\n",
    "\n",
    "normal_dist2 = np.random.randn(10000)\n",
    "normal_df2 = pd.DataFrame({'value' : normal_dist2})\n",
    "# Create a Pandas Series for easy sample function\n",
    "normal_dist2 = pd.Series(normal_dist)\n",
    "\n",
    "normal_df_total = pd.DataFrame({'value1' : normal_dist, \n",
    "                                'value2' : normal_dist2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "bf454e36159ab26c2cb7229cd8de96b3c4999dc7"
   },
   "outputs": [],
   "source": [
    "# Scatterplot\n",
    "plt.figure(figsize=(18,5))\n",
    "sns.scatterplot(data=normal_df)\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.title('Scatterplot of a Normal Distribution', fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "e5bdaa0de445feef0f71a36b147bf1b676852c7e"
   },
   "outputs": [],
   "source": [
    "# Normal Distribution as a Bell Curve\n",
    "plt.figure(figsize=(18,5))\n",
    "sns.distplot(normal_df)\n",
    "plt.title('Normal distribution (n=1000)', fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "216d5331a327f81ef7d340bd6380d5ce681cf430"
   },
   "source": [
    "### Binomial Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "92cbeff5b8b7cd396f6c941e603a3ec525f0cc0f"
   },
   "source": [
    "A Binomial Distribution has a countable number of outcomes and is therefore discrete.\n",
    "\n",
    "Binomial distributions must meet the following three criteria:\n",
    "\n",
    "1. The number of observations or trials is fixed. In other words, you can only figure out the probability of something happening if you do it a certain number of times.\n",
    "2. Each observation or trial is independent. In other words, none of your trials have an effect on the probability of the next trial.\n",
    "3. The probability of success is exactly the same from one trial to another.\n",
    "\n",
    "An intuitive explanation of a binomial distribution is flipping a coin 10 times. If we have a fair coin our chance of getting heads (p) is 0.50. Now we throw the coin 10 times and count how many times it comes up heads. In most situations we will get heads 5 times, but there is also a change that we get heads 9 times. The PMF of a binomial distribution will give these probabilities if we say N = 10 and p = 0.5. We say that the x for heads is 1 and 0 for tails.\n",
    "\n",
    "PMF:\n",
    "\n",
    "![](http://reliabilityace.com/formulas/binomial-pmf.png)\n",
    "\n",
    "CDF:\n",
    "\n",
    "![](http://reliabilityace.com/formulas/binomial-cpf.png)\n",
    "\n",
    "\n",
    "A **Bernoulli Distribution** is a special case of a Binomial Distribution.\n",
    "\n",
    "All values in a Bernoulli Distribution are either 0 or 1. \n",
    "\n",
    "For example, if we take an unfair coin which falls on heads 60 % of the time, we can describe the Bernoulli distribution as follows:\n",
    "\n",
    "p (change of heads) = 0.6\n",
    "\n",
    "1 - p (change of tails) = 0.4\n",
    "\n",
    "heads = 1\n",
    "\n",
    "tails = 0\n",
    "\n",
    "Formally, we can describe a Bernoulli distribution with the following PMF (Probability Mass Function):\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/a9207475ab305d280d2958f5c259f996415548e9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d50cc50740b32b4aee6e72ba7e6de849bd6f0e89"
   },
   "outputs": [],
   "source": [
    "# Change of heads (outcome 1)\n",
    "p = 0.6\n",
    "\n",
    "# Create Bernoulli samples\n",
    "bern_dist = bernoulli.rvs(p, size=1000)\n",
    "bern_df = pd.DataFrame({'value' : bern_dist})\n",
    "bern_values = bern_df['value'].value_counts()\n",
    "\n",
    "# Plot Distribution\n",
    "plt.figure(figsize=(18,4))\n",
    "bern_values.plot(kind='bar', rot=0)\n",
    "plt.annotate(xy=(0.85,300), \n",
    "             s='Samples that came up Tails\\nn = {}'.format(bern_values[0]), \n",
    "             fontsize='large', \n",
    "             color='white')\n",
    "plt.annotate(xy=(-0.2,300), \n",
    "             s='Samples that came up Heads\\nn = {}'.format(bern_values[1]), \n",
    "             fontsize='large', \n",
    "             color='white')\n",
    "plt.title('Bernoulli Distribution: p = 0.6, n = 1000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b6a5cb2a8e2963924420549806a57a53921b5c0"
   },
   "source": [
    "### Poisson Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b15493ae16821b00e67d687b91a6fd422ff50d5"
   },
   "source": [
    "The Poisson distribution is a discrete distribution and is popular for modelling the number of times an event occurs in an interval of time or space. \n",
    "\n",
    "It takes a value lambda, which is equal to the mean of the distribution.\n",
    "\n",
    "PMF: \n",
    "\n",
    "![](https://study.com/cimages/multimages/16/poisson1a.jpg)\n",
    "\n",
    "CDF: \n",
    "![](http://www.jennessent.com/images/cdf_poisson.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b271f5d14668ec9af1c978869a5ee4439c5526cf"
   },
   "outputs": [],
   "source": [
    "x = np.arange(0, 20, 0.1)\n",
    "y = np.exp(-5)*np.power(5, x)/factorial(x)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.title('Poisson distribution with lambda=5', fontsize='xx-large')\n",
    "plt.plot(x, y, 'bs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "19b51633191227da35347fd81dd1f23723e12fd3"
   },
   "source": [
    "### Log-Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c13f97c1551a715fc3cfa3a6d634ff8485280d41"
   },
   "source": [
    "A log-normal distribution is continuous. The main characteristic of a log-normal distribution is that it's logarithm is normally distributed. It is also referred to as Galton's distribution.\n",
    "\n",
    "PDF: \n",
    "\n",
    "![](https://www.mhnederlof.nl/images/lognormaldensity.jpg)\n",
    "\n",
    "CDF:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/29095d9cbd6539833d549c59149b9fc5bd06339b)\n",
    "\n",
    "Where Phi is the CDF of the standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "953c19e8e5ac877f598f48e363db57836d48432a"
   },
   "outputs": [],
   "source": [
    "# Specify standard deviation and mean\n",
    "std = 1\n",
    "mean = 5\n",
    "\n",
    "# Create log-normal distribution\n",
    "dist=lognorm(std,loc=mean)\n",
    "x=np.linspace(0,15,200)\n",
    "\n",
    "# Visualize log-normal distribution\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.xlim(5, 10)\n",
    "plt.plot(x,dist.pdf(x), label='log-normal PDF')\n",
    "plt.plot(x,dist.cdf(x), label='log-normal CDF')\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.title('Visualization of log-normal PDF and CDF', fontsize='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88a267e724f432804693641ec2c25528a2fd7b93"
   },
   "source": [
    "## Summary Statistics and Moments <a id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a2801c16ab5ec6338cc5accedb40197af6c836e8"
   },
   "source": [
    "**Mean, Median and Mode** \n",
    "\n",
    "Note: The mean is also called the first moment.\n",
    "\n",
    "\n",
    "![](https://qph.fs.quoracdn.net/main-qimg-29a4925034e075f16e1c743a4b3dda8b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8729c8e7dfc2a4fe899ffb6a86d73da7c6467cb5"
   },
   "source": [
    "### Moments\n",
    "\n",
    "A moment is a quantitative measure that says something about the shape of a distribution. There are central moments and non-central moments. This section is focused on the central moments.\n",
    "\n",
    "The 0th central moment is the total probability and is always equal to 1.\n",
    "\n",
    "The 1st moment is the mean (expected value).\n",
    "\n",
    "The 2nd central moment is the variance.\n",
    "\n",
    "**Variance** = The average of the squared distance of the mean. Variance is interesting in a mathematical sense, but the standard deviation is often a much better measure of how spread out the distribution is.\n",
    "\n",
    "![](http://www.visualmining.com/wp-content/uploads/2013/02/analytics_formula_variance.png)\n",
    "\n",
    "**Standard Deviation** = The square root of the variance\n",
    "\n",
    "![](http://www.visualmining.com/wp-content/uploads/2013/02/analytics_formula_std_dev.png)\n",
    "\n",
    "The 3rd central moment is the skewness.\n",
    "\n",
    "**Skewness** = A measure that describes the contrast of one tail versus the other tail. For example, if there are more high values in your distribution than low values then your distribution is 'skewed' towards the high values.\n",
    "\n",
    "![](http://www.visualmining.com/wp-content/uploads/2013/02/analytics_formula_skewness.png)\n",
    "\n",
    "The 4th central moment is the kurtosis.\n",
    "\n",
    "**Kurtosis** = A measure of how 'fat' the tails in the distribution are.\n",
    "\n",
    "![](http://www.visualmining.com/wp-content/uploads/2013/02/analytics_formula_kurtosis.png)\n",
    "\n",
    "The higher the moment, the harder it is to estimate with samples. Larger samples are required in order to obtain good estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5bca8bcc3aa295113f838fa36298a07442dce08"
   },
   "outputs": [],
   "source": [
    "# Summary\n",
    "print('Summary Statistics for a normal distribution: ')\n",
    "# Median\n",
    "medi = median(normal_dist)\n",
    "print('Median: ', medi)\n",
    "display(normal_df.describe())\n",
    "\n",
    "# Standard Deviation\n",
    "std = sqrt(np.var(normal_dist))\n",
    "\n",
    "print('The first four calculated moments of a normal distribution: ')\n",
    "# Mean\n",
    "mean = normal_dist.mean()\n",
    "print('Mean: ', mean)\n",
    "\n",
    "# Variance\n",
    "var = np.var(normal_dist)\n",
    "print('Variance: ', var)\n",
    "\n",
    "# Return unbiased skew normalized by N-1\n",
    "skew = normal_df['value'].skew()\n",
    "print('Skewness: ', skew)\n",
    "\n",
    "# Return unbiased kurtosis over requested axis using Fisher’s definition of kurtosis \n",
    "# (kurtosis of normal == 0.0) normalized by N-1\n",
    "kurt = normal_df['value'].kurtosis()\n",
    "print('Kurtosis: ', kurt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b360ab8eefb8268db7430cfc45364fd700d1aa0"
   },
   "source": [
    "## Bias, MSE and SE <a id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de3f98a8a8c46e22678e26c0c5847c9e4503479c"
   },
   "source": [
    "**Bias** is a measure of how far the sample mean deviates from the population mean. The sample mean is also called **Expected value**.\n",
    "\n",
    "Formula for Bias:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/82a9c6501a54260ed0edd2f03923719b9f2db906)\n",
    "\n",
    "The formula for expected value (EV) makes it apparent that the bias can also be formulated as the expected value minus the population mean:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/12828b1f927b39d2fa9d75f82c02b91209182911)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7de089fa73a9b42aeab7a1d78e4b2856a6d9527f"
   },
   "outputs": [],
   "source": [
    "# Take sample\n",
    "normal_df_sample = normal_df.sample(100)\n",
    "\n",
    "# Calculate Expected Value (EV), population mean and bias\n",
    "ev = normal_df_sample.mean()[0]\n",
    "pop_mean = normal_df.mean()[0]\n",
    "bias = ev - pop_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "268ea18ed84402103384034862ed14bbe43c6e19"
   },
   "outputs": [],
   "source": [
    "print('Sample mean (Expected Value): ', ev)\n",
    "print('Population mean: ', pop_mean)\n",
    "print('Bias: ', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dcb382e5b8587d277a88269f9a1e4760ffbc21d8"
   },
   "source": [
    "**MSE (Mean Squared Error)** is a formula to measure how much estimators deviate from the true distribution. This can be very useful with for example, evaluating regression models.\n",
    "\n",
    "\n",
    "![](https://i.stack.imgur.com/iSWyZ.png)\n",
    "\n",
    "\n",
    "**RMSE (Root Mean Squared Error)** is just the root of the MSE.\n",
    "\n",
    "\n",
    "![](http://file.scirp.org/Html/htmlimages/5-2601289x/fcdba7fc-a40e-4019-9e95-aca3dc2db149.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "839b15a284e146ae9f582c28ab6e30af07eecd86"
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "Y = 100 # Actual Value\n",
    "YH = 94 # Predicted Value\n",
    "\n",
    "# MSE Formula \n",
    "def MSE(Y, YH):\n",
    "     return np.square(YH - Y).mean()\n",
    "\n",
    "# RMSE formula\n",
    "def RMSE(Y, YH):\n",
    "    return sqrt(np.square(YH - Y).mean())\n",
    "\n",
    "\n",
    "print('MSE: ', MSE(Y, YH))\n",
    "\n",
    "print('RMSE: ', RMSE(Y, YH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f51ce7f24700f38a575a0236e9f40964d16a01f5"
   },
   "source": [
    "The **Standard Error (SE)** measures how spread the distribution is from the sample mean.\n",
    "\n",
    "![](http://desktopia.net/p/2018/07/standard-deviation-biology-for-life-in-standard-error-of-the-mean-formula.gif)\n",
    "\n",
    "The formula can also be defined as the standard deviation divided by the square root of the number of samples.\n",
    "\n",
    "![](https://toptipbio.com/wp-content/uploads/2017/07/Standard-error-formula.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9b07a5750071eaba369eb243fa4f8227886cf9e4"
   },
   "outputs": [],
   "source": [
    "# Standard Error (SE)\n",
    "uni_sample = uniform_dist.sample(100)\n",
    "norm_sample = normal_dist.sample(100)\n",
    "\n",
    "print('Standard Error of uniform sample: ', sem(uni_sample))\n",
    "print('Standard Error of normal sample: ', sem(norm_sample))\n",
    "\n",
    "# The random samples from the normal distribution should have a higher standard error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b4607505a753b3942153441a688ce20afccedc8"
   },
   "source": [
    "## Sampling methods <a id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6168c5bd2265b98c5b2f8b6b97de04148b51da58"
   },
   "source": [
    "**Non-Representative Sampling:**\n",
    "\n",
    "Convenience Sampling = Pick samples that are most convenient, like the top of a shelf or people that can be easily approached.\n",
    "\n",
    "Haphazard Sampling = Pick samples without thinking about it. This often gives the illusion take you are picking out samples at random. \n",
    "\n",
    "Purposive Sampling = Pick samples for a specific purpose. An example is to focus on extreme cases. This can be useful but is limited because it doesn't allow you to make statements about the whole population.\n",
    "\n",
    "**Representative Sampling:**\n",
    "\n",
    "Simple Random Sampling = Pick samples (psuedo)randomly.\n",
    "\n",
    "Systematic Sampling = Pick samples with a fixed interval. For example every 10th sample (0, 10, 20, etc.).\n",
    "\n",
    "Stratified Sampling = Pick the same amount of samples from different groups (strata) in the population.\n",
    "\n",
    "Cluster Sampling = Divide the population into groups (clusters) and pick samples from those groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b2e6a3d3383b57faaef48df34707de14577a2ab"
   },
   "outputs": [],
   "source": [
    "# Note that we take very small samples just to illustrate the different sampling methods\n",
    "\n",
    "print('---Non-Representative samples:---\\n')\n",
    "# Convenience samples\n",
    "con_samples = normal_dist[0:5]\n",
    "print('Convenience samples:\\n\\n{}\\n'.format(con_samples))\n",
    "\n",
    "# Haphazard samples (Picking out some numbers)\n",
    "hap_samples = [normal_dist[12], normal_dist[55], normal_dist[582], normal_dist[821], normal_dist[999]]\n",
    "print('Haphazard samples:\\n\\n{}\\n'.format(hap_samples))\n",
    "\n",
    "# Purposive samples (Pick samples for a specific purpose)\n",
    "# In this example we pick the 5 highest values in our distribution\n",
    "purp_samples = normal_dist.nlargest(n=5)\n",
    "print('Purposive samples:\\n\\n{}\\n'.format(purp_samples))\n",
    "\n",
    "print('---Representative samples:---\\n')\n",
    "\n",
    "# Simple (pseudo)random sample\n",
    "rand_samples = normal_dist.sample(5)\n",
    "print('Random samples:\\n\\n{}\\n'.format(rand_samples))\n",
    "\n",
    "# Systematic sample (Every 2000th value)\n",
    "sys_samples = normal_dist[normal_dist.index % 2000 == 0]\n",
    "print('Systematic samples:\\n\\n{}\\n'.format(sys_samples))\n",
    "\n",
    "# Stratified Sampling\n",
    "# We will get 1 person from every city in the dataset\n",
    "# We have 8 cities so that makes a total of 8 samples\n",
    "df = pd.read_csv(KAGGLE_DIR + 'toy_dataset.csv')\n",
    "\n",
    "strat_samples = []\n",
    "\n",
    "for city in df['City'].unique():\n",
    "    samp = df[df['City'] == city].sample(1)\n",
    "    strat_samples.append(samp['Income'].item())\n",
    "    \n",
    "print('Stratified samples:\\n\\n{}\\n'.format(strat_samples))\n",
    "\n",
    "# Cluster Sampling\n",
    "# Make random clusters of ten people (Here with replacement)\n",
    "c1 = normal_dist.sample(10)\n",
    "c2 = normal_dist.sample(10)\n",
    "c3 = normal_dist.sample(10)\n",
    "c4 = normal_dist.sample(10)\n",
    "c5 = normal_dist.sample(10)\n",
    "\n",
    "# Take sample from every cluster (with replacement)\n",
    "clusters = [c1,c2,c3,c4,c5]\n",
    "cluster_samples = []\n",
    "for c in clusters:\n",
    "    clus_samp = c.sample(1)\n",
    "    cluster_samples.extend(clus_samp)\n",
    "print('Cluster samples:\\n\\n{}'.format(cluster_samples))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d6e2d0bd983e68e86af13af1ad4e5f3e973c0193"
   },
   "source": [
    "## Covariance <a id=\"7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d182cf7e089fccdc808b0c0938bdf1938a7cfb7"
   },
   "source": [
    "Covariance is a measure of how much two random variables vary together. variance is similar to covariance in that variance shows you how much one variable varies. Covariance tells you how two variables vary together.\n",
    "\n",
    "If two variables are independent, their covariance is 0. However, a covariance of 0 does not imply that the variables are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "7a795570fae61547decb79d331479d3098701aaa"
   },
   "outputs": [],
   "source": [
    "# Covariance between Age and Income\n",
    "print('Covariance between Age and Income: ')\n",
    "\n",
    "df[['Age', 'Income']].cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "45baa36045e88dbab22e850e4df6b4420fff747f"
   },
   "source": [
    "## Correlation <a id=\"8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "118f9e24a69065a48efb5c78ac03c6c475a1dad9"
   },
   "source": [
    "Correlation is a standardized version of covariance. Here it becomes more clear that Age and Income do not have a strong correlation in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "634cdc7963624e398ab1841ccbbf00be08c12948"
   },
   "source": [
    "The formula for Pearson's correlation coefficient consists of the covariance between the two random variables divided by the standard deviation of the first random variable times the standard deviation of the second random variable.\n",
    "\n",
    "Formula for Pearson's correlation coefficient:\n",
    "\n",
    "![](http://sherrytowers.com/wp-content/uploads/2013/09/correlation_xy-300x97.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b14ff3e57239585b05b34a01f731dc52eec8ea94"
   },
   "outputs": [],
   "source": [
    "# Correlation between two normal distributions\n",
    "# Using Pearson's correlation\n",
    "print('Pearson: ')\n",
    "df[['Age', 'Income']].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "81ebd4c6e6be66529a181dcf5e3946ab051d04b6"
   },
   "source": [
    "Another method for calculating a correlation coefficient is 'Spearman's Rho'. The formula looks different but it will give similar results as Pearson's method. In this example we see almost no difference, but this is partly because it is obvious that the Age and Income columns in our dataset have no correlation.\n",
    "\n",
    "Formula for Spearmans Rho:\n",
    "\n",
    "![](http://s3.amazonaws.com/hm_120408/fa/3d86/yhf5/9dwq/4m6e2kcav/original.jpg?1447778688)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9f94042e40978a8bd3df01a828d02dd25ca0db70"
   },
   "outputs": [],
   "source": [
    "# Using Spearman's rho correlation\n",
    "print('Spearman: ')\n",
    "df[['Age', 'Income']].corr(method='spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79a0d6a9333121c08351ceaa76bf661b661f124a"
   },
   "source": [
    "## Linear regression <a id=\"9\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "742e13d2528cff6398f9016b80d01483d2bf7c2c"
   },
   "source": [
    "Linear Regression can be performed through Ordinary Least Squares (OLS) or Maximum Likelihood Estimation (MLE).\n",
    "\n",
    "Most Python libraries use OLS to fit linear models.\n",
    "\n",
    "![](https://image.slidesharecdn.com/simplelinearregressionpelatihan-090829234643-phpapp02/95/simple-linier-regression-9-728.jpg?cb=1251589640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c4b58d1312d3e4a28244dc4d5640426359859161"
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "x = np.random.uniform(low=20, high=260, size=100)\n",
    "y = 50000 + 2000*x - 4.5 * x**2 + np.random.normal(size=100, loc=0, scale=10000)\n",
    "\n",
    "# Plot data with Linear Regression\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.title('Well fitted but not well fitting: Linear regression plot on quadratic data', fontsize='xx-large')\n",
    "sns.regplot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1029f440d9b10cfe187df7a19b3d34a71c02520b"
   },
   "source": [
    "Here we observe that the linear model is well-fitted. However, a linear model is probably not ideal for our data, because the data follows a quadratic pattern. A [polynomial regression model](https://en.wikipedia.org/wiki/Polynomial_regression) would better fit the data, but this is outside the scope of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8428a93896226b94dd726413a98074953eb90a5c"
   },
   "source": [
    "We can also implement linear regression with a bare-bones approach. In the following example we measure the vertical distance and horizontal distance between a random data point and the regression line. \n",
    "\n",
    "For more information on implementing linear regression from scratch [I highly recommend this explanation by Luis Serrano](https://aitube.io/video/introduction-to-linear-regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cdee8f073327f0be4641f746be826462a9bae64c"
   },
   "outputs": [],
   "source": [
    "# Linear regression from scratch\n",
    "import random\n",
    "# Create data from regression\n",
    "xs = np.array(range(1,20))\n",
    "ys = [0,8,10,8,15,20,26,29,38,35,40,60,50,61,70,75,80,88,96]\n",
    "\n",
    "# Put data in dictionary\n",
    "data = dict()\n",
    "for i in list(xs):\n",
    "    data.update({xs[i-1] : ys[i-1]})\n",
    "\n",
    "# Slope\n",
    "m = 0\n",
    "# y intercept\n",
    "b = 0\n",
    "# Learning rate\n",
    "lr = 0.0001\n",
    "# Number of epochs\n",
    "epochs = 100000\n",
    "\n",
    "# Formula for linear line\n",
    "def lin(x):\n",
    "    return m * x + b\n",
    "\n",
    "# Linear regression algorithm\n",
    "for i in range(epochs):\n",
    "    # Pick a random point and calculate vertical distance and horizontal distance\n",
    "    rand_point = random.choice(list(data.items()))\n",
    "    vert_dist = abs((m * rand_point[0] + b) - rand_point[1])\n",
    "    hor_dist = rand_point[0]\n",
    "\n",
    "    if (m * rand_point[0] + b) - rand_point[1] < 0:\n",
    "        # Adjust line upwards\n",
    "        m += lr * vert_dist * hor_dist\n",
    "        b += lr * vert_dist   \n",
    "    else:\n",
    "        # Adjust line downwards\n",
    "        m -= lr * vert_dist * hor_dist\n",
    "        b -= lr * vert_dist\n",
    "        \n",
    "# Plot data points and regression line\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.scatter(data.keys(), data.values())\n",
    "plt.plot(xs, lin(xs))\n",
    "plt.title('Linear Regression result')  \n",
    "print('Slope: {}\\nIntercept: {}'.format(m, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0a56f6f9ef61c5cef49235c70b832843a32fca98"
   },
   "source": [
    "The coefficients of a linear model can also be computed using MSE (Mean Squared Error) without an iterative approach. I implemented Python code for this technique as well. The code is in [the second cell of this Github repository](https://github.com/CarloLepelaars/linreg/blob/master/linreg_from_scratch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e9ffbd3ff025ed57a1b2c5829d4afbba01fa4ce"
   },
   "source": [
    "### Anscombe's Quartet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4b044b5a175d781fdea7122ef9846dd583287c12"
   },
   "source": [
    "Anscombe's quartet is a set of four datasets that have the same descriptive statistics and linear regression fit. The datasets are however very different from each other.\n",
    "\n",
    "This sketches the issue that although summary statistics and regression models are really helpful in understanding your data, you should always visualize the data to see whats really going on. It also shows that a few outliers can really mess up your model.\n",
    "\n",
    "[More information on Anscombe's Quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7abc7a2bd4f3475404b4a188a8b0267a51e2f09"
   },
   "source": [
    "![](https://www.researchgate.net/profile/Arch_Woodside2/publication/286454889/figure/fig3/AS:669434310037526@1536616985820/Anscombes-quartet-of-different-XY-plots-of-four-data-sets-having-identical-averages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c36c7fa23a988beb6c1d57d3b9b62ea3173860ee"
   },
   "source": [
    "## Bootstrapping <a id=\"10\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52e9569f2fad4ddfaf0d282986c82c9a60a723e1"
   },
   "source": [
    "Bootstrapping is a resampling technique to quantify the uncertainty of an estimator given sample data. In other words, we have a sample of data and we take multiple samples from that sample. For example, with bootstrapping we can take means for each bootstrap sample and thereby make a distribution of means.\n",
    "\n",
    "Once we created a distribution of estimators we can use this to make decisions. \n",
    "\n",
    "Bootstrapping can be:\n",
    "1. Non-parametric (Take random samples from sample)\n",
    "2. Parametric (Take from a (normal) distribution with sample mean and variance).\n",
    "    Downside: You are making assumptions about the distribution.\n",
    "    Upside: Computationally more light\n",
    "3. Online bootstrap (Take samples from a stream of data)\n",
    "\n",
    "The following code implements a simple non-parametric bootstrap to create a distribution of means, medians and midranges of the Income distribution in our Toy Dataset. We can use this to deduce which income means will make sense for subsequent samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8120d915cd1c9a701802cf6e377d719f466b9518"
   },
   "outputs": [],
   "source": [
    "# scikit-learn bootstrap package\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# data sample\n",
    "data = df['Income']\n",
    "\n",
    "# prepare bootstrap samples\n",
    "boot = resample(data, replace=True, n_samples=5, random_state=1)\n",
    "print('Bootstrap Sample: \\n{}\\n'.format(boot))\n",
    "print('Mean of the population: ', data.mean())\n",
    "print('Standard Deviation of the population: ', data.std())\n",
    "\n",
    "# Bootstrap plot\n",
    "pd.plotting.bootstrap_plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8ea3cd21d40a1e748551fa791ef25a92fae006e6"
   },
   "source": [
    "## Hypothesis testing <a id=\"11\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13403a6b1a131e7a9a8182a18d8f01c2abcb12bd"
   },
   "source": [
    "We establish two hypotheses, H0 (Null hypothesis) and Ha (Alternative Hypothesis). \n",
    "\n",
    "We can make four different decisions with hypothesis testing:\n",
    "1. Reject H0 and H0 is not true (no error)\n",
    "2. Do not reject H0 and H0 is true (no error)\n",
    "3. Reject H0 and H0 is true (Type 1 Error)\n",
    "4. Do not reject H0 and H0 is not true (Type 2 error)\n",
    "\n",
    "Type 1 error is also called Alpha error.\n",
    "Type 2 error is also called Beta error.\n",
    "\n",
    "![](https://qph.fs.quoracdn.net/main-qimg-84121cf5638cbb5919999b2a8d928c91)\n",
    "\n",
    "![](https://i.stack.imgur.com/x1GQ1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "08e7d2a11daa24dc208d06affb27cf7e9200982c"
   },
   "source": [
    "### P-Value\n",
    "\n",
    "A p-value is the probability of finding equal or more extreme results when the null hyptohesis (H0) is true. In other words, a low p-value means that we have compelling evidence to reject the null hypothesis.\n",
    "\n",
    "If the p-value is lower than 5% (p < 0.05). We often reject H0 and accept Ha is true. We say that p < 0.05 is statistically significant, because there is less than 5% chance that we are wrong in rejecting the null hypothesis.\n",
    "\n",
    "One way to calculate the p-value is through a T-test. We can use [Scipy's ttest_ind function](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html) to calculate the t-test for the means of two independent samples of scores. In this example we calculate the t-statistic and p-value of two random samples 10 times. \n",
    "\n",
    "We see that the p-value is sometimes very low, but this does not mean that these two random samples are correlated. This is why you have to be careful with relying too heavily of p-values. If you repeat an experiment multiple times you can get trapped in the illusion that there is correlation where there is only randomness.\n",
    "\n",
    "[This xkcd comic perfectly illustrates the hazards of relying too much on p-values](https://xkcd.com/882/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b4ace3b99393063a70412c33d63f1d0707aa6d3"
   },
   "outputs": [],
   "source": [
    "# Perform t-test and compute p value of two random samples\n",
    "print('T-statistics and p-values of two random samples.')\n",
    "for _ in range(10):\n",
    "    rand_sample1 = np.random.random_sample(10)\n",
    "    rand_sample2 = np.random.random_sample(10)\n",
    "    print(stats.ttest_ind(rand_sample1, rand_sample2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "69cb46f4ac2f23933cd2491430d854f15c429ee4"
   },
   "outputs": [],
   "source": [
    "# To-do\n",
    "# Equivalence testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d6b36a47d2949d96d2420746ddafa996eb3afb9"
   },
   "source": [
    "### q-q plot (quantile-quantile plot)\n",
    "\n",
    "Many statistical techniques require that data is coming from a normal distribution (for example, t-test). Therefore, it is important to verify this before applying statistical techniques.\n",
    "\n",
    "One approach is to visualize and make a judgment about the distribution. A q-q plot is very helpful for determining if a distribution is normal. There are other tests for testing 'normality', but this is beyond the scope of this tutorial.\n",
    "\n",
    "In the first plot we can easily see that the values line up nicely. From this we conclude that the data is normally distributed.\n",
    "\n",
    "In the second plot we can see that the values don't line up. Our conclusion is that the data is not normally distributed. In this case the data was uniformly distributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "333535ddf110422ac040ba3b8cf2a8b5fa518882"
   },
   "outputs": [],
   "source": [
    "# q-q plot of a normal distribution\n",
    "plt.figure(figsize=(15,6))\n",
    "stats.probplot(normal_dist, dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "56b503c824e9bf4b44cf56419228044016e74c71"
   },
   "outputs": [],
   "source": [
    "# q-q plot of a uniform/random distribution\n",
    "plt.figure(figsize=(15,6))\n",
    "stats.probplot(uniform_dist, dist=\"norm\", plot=plt) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1dd800099831fbc44ad8a23446558c5962e1fb0d"
   },
   "source": [
    "## Outliers <a id=\"12\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "87c3128d6ef6cac8a8193c771398c84e396db522"
   },
   "source": [
    "An outlier is an observation which deviates from other observations. An outlier often stands out and could be an error.\n",
    "\n",
    "Outliers can mess up you statistical models. However, outliers should only be removed when you have established good reasons for removing the outlier.\n",
    "\n",
    "Sometimes the outliers are the main topic of interest. This is for example the case with fraud detection. There are many outlier detection methods, but here we will discuss Grubbs test and Tukey’s method. Both tests assume that the data is normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dff3bf6ce46e3291e3041d663890f355d5ee0f48"
   },
   "source": [
    "### Grubbs Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "883dc5e9ad3b2a116688073e2a1143569b068f19"
   },
   "source": [
    "In Grubbs test, the null hypothesis is that no observation is an outlier, while the alternative hypothesis is that there is one observation an outlier. Thus the Grubbs test is only searching for one outlier observation.\n",
    "\n",
    "The formula for Grubbs test:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/bafc310f1dbca658728c73256fed19b6a7f11130)\n",
    "\n",
    "Where Y_hat is the sample mean and s is the standard deviation. The Grubbs test statistic is the largest absolute deviation from the sample mean in units of the sample standard deviation.\n",
    "\n",
    "[Source](https://en.wikipedia.org/wiki/Grubbs%27_test_for_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1084114bebc3071ade30f4c76bcec634a749ea1"
   },
   "source": [
    "### Tukey's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "100c85da9f7426cf496cbefaab5d34f4b3ae7725"
   },
   "source": [
    "Tukey suggested that an observation is an outlier whenever an observation is 1.5 times the interquartile range below the first quartile or 1.5 times the interquartile range above the third quartile. This may sound complicated, but is quite intuitive if you see it visually.\n",
    "\n",
    "For normal distributions, Tukey’s criteria for outlier observations is unlikely if no outliers are present, but using Tukey’s criteria for other distributions should be taken with a grain of salt.\n",
    "\n",
    "The formula for Tukey's method:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/2a103bbd9233d9f8f711a7c76dfeb9694446f860)\n",
    "\n",
    "Ya is the larger of two means being compared. SE is the standard error of the sum of the means.\n",
    "\n",
    "[Source](https://en.wikipedia.org/wiki/Tukey%27s_range_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "9c88cbf675f80001ab59301473e91ce02124ebce"
   },
   "outputs": [],
   "source": [
    "# Detect outliers on the 'Income' column of the Toy Dataset\n",
    "\n",
    "# Function for detecting outliers a la Tukey's method using z-scores\n",
    "def tukey_outliers(data) -> list:\n",
    "    # For more information on calculating the threshold check out:\n",
    "    # https://medium.com/datadriveninvestor/finding-outliers-in-dataset-using-python-efc3fce6ce32\n",
    "    threshold = 3\n",
    "    \n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    \n",
    "    # Spot and collect outliers\n",
    "    outliers = []\n",
    "    for i in data:\n",
    "        z_score = (i - mean) / std\n",
    "        if abs(z_score) > threshold:\n",
    "            outliers.append(i)\n",
    "    return outliers\n",
    "\n",
    "# Get outliers\n",
    "income_outliers = tukey_outliers(df['Income'])\n",
    "\n",
    "# Visualize distribution and outliers\n",
    "plt.figure(figsize=(15,6))\n",
    "df['Income'].plot(kind='hist', bins=100, label='Income distribution')\n",
    "plt.hist(income_outliers, bins=20, label='Outliers')\n",
    "plt.title(\"Outlier detection a la Tukey's method\", fontsize='xx-large')\n",
    "plt.xlabel('Income')\n",
    "plt.legend(fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b09aa988cc8eaabf1474b14c943b6743f2e5345d"
   },
   "source": [
    "## Overfitting <a id=\"20\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7437e4efd678ed951eaf85e36a7062423c60738f"
   },
   "source": [
    "Our model is overfitting if it is also modeling the 'noise' in the data. This implies that the model will not generalize well to new data even though the error on the training data becomes very small. Linear models are unlikely to overfit, but as models become more flexible we have to be wary of overfitting. Our model can also underfit which means that it has a large error on the training data. \n",
    "\n",
    "Finding the sweet spot between overfitting and underfitting is called the [Bias Variance Trade-off](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). It is nice to know this theorem, but more important to understand how to prevent it. I will explain some techniques for how to do this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d9a9d76f87a8187a4fdc147b2157bbafb36cab96"
   },
   "source": [
    "### Prevention of Overfitting\n",
    "\n",
    "1. Split data into training data and test data.\n",
    "2. Regularization: limit the flexibility of the model.\n",
    "3. Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d507cc66b01e81773eda7635b471886db1bac7c2"
   },
   "source": [
    "### Cross Validation\n",
    "\n",
    "Cross validation is a technique to estimate the accuracy of our statistical model. It is also called out-of-sample testing or rotation estimation. Cross validation will help us to recognize overfitting and to check if our model generalizes to new (out-of-sample) data.\n",
    "\n",
    "A popular cross validation technique is called k-fold cross validation. The idea is simple, we split our dataset up in k datasets and out of each dataset k we pick out a few samples. We then fit our model on the rest of k and try to predict the samples we picked out. We use a metric like Mean Squared Error to estimate how good our predictions are. This procedure is repeated and then we look at the average of the predictions over multiple cross-validation data sets. \n",
    "\n",
    "A special case where we pick out one samples is called 'Leave-One-Out Cross Validation (LOOCV)'. However, the variance of LOOCV is high.\n",
    "\n",
    "For more information about cross validation [check out this blog](https://machinelearningmastery.com/k-fold-cross-validation/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "73d2f5b676a9dc9e846583d7cb4b4854f8403aae"
   },
   "source": [
    "## Generalized Linear Models (GLMs) <a id=\"13\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6a9d976f5dac6fda7d270d9a3434f89b15d5efd7"
   },
   "source": [
    "### Link functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4b02c0ff83e430080dd9a22b6d07c41c16ea17d9"
   },
   "source": [
    "A Link Function is used in Generalized Linear Models (GLMs) to apply linear models for a continuous response variable given continuous and/or categorical predictors. A link function that is often used is called the inverse logit or logistic sigmoid function.\n",
    "\n",
    "The link function provides a relationship between the linear predictor and the mean of a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7007fc1956383c846ff0aa3305928e3a7fbf34d"
   },
   "outputs": [],
   "source": [
    "# Inverse logit function (link function)\n",
    "def inv_logit(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "t1 = np.arange(-10, 10, 0.1)\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(t1, inv_logit(t1), \n",
    "         t1, inv_logit(t1-2),   \n",
    "         t1, inv_logit(t1*2))\n",
    "plt.title('Inverse logit functions', fontsize='xx-large')\n",
    "plt.legend(('Normal', 'Changed intercept', 'Changed slope'), fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9e7a345d4f8feb09047c6ca65dd58f8be09168cc"
   },
   "source": [
    "### Logistic regression\n",
    "\n",
    "With logistic regression we use a link function like the inverse logit function mentioned above to model a binary dependent variable. While a linear regression model predicts the expected value of y given x directly, a GLM uses a link function. \n",
    "\n",
    "We can easily implement logistic regression with [sklearn's Logistic Regression function.](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "644dba4dc33e3b31b827686bc2ccbf67af8968da"
   },
   "outputs": [],
   "source": [
    "# Simple example of Logistic Regression in Python\n",
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Logistic regression classifier\n",
    "clf = LogisticRegression(random_state=0, \n",
    "                         solver='lbfgs',\n",
    "                         multi_class='multinomial').fit(X, y)\n",
    "\n",
    "print('Accuracy score of logistic regression model on the Iris flower dataset: {}'.format(clf.score(X, y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3896926fe19b90be5ceef2262caff7f39ea2e908"
   },
   "source": [
    "## Frequentist vs. Bayes <a id=\"14\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e9dfef65996fabe3913cdc72e9b01683bc85f175"
   },
   "source": [
    "Frequentist:\n",
    "\n",
    "1. Fixed parameters (Processes are fixed)\n",
    "2. Repeated sampling -> Probabilities\n",
    "\n",
    "Bayes:\n",
    "\n",
    "1. Probability as \"degree of belief\"\n",
    "2. P(parameter) -> All plausible values of the parameter\n",
    "3. Updates degree of belief based on a prior belief\n",
    "\n",
    "\n",
    "Frequentists and Bayesians agree that Bayes' Theorem is valid. See figure below for explanation of this theorem.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*LB-G6WBuswEfpg20FMighA.png)\n",
    "\n",
    "\n",
    "Bayes theorem extends to distributions and random variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "36f9e0d7ec42a5efc8df994527f946b7e688e591"
   },
   "source": [
    "# The end!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9b4ea287e40ad372985db795f7968cdfa3221874"
   },
   "source": [
    "**If you like this Kaggle kernel, feel free to give an upvote and leave a comment! I will try to implement your suggestions in this kernel!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b29354e2162921d1a3bacffad073248421e0577"
   },
   "source": [
    "## Bonus: Free statistics courses <a id=\"15\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1120167e3a0516a1d9f7f357e4b38cac3687200e"
   },
   "source": [
    "There is a lot of free material online for people who want to dive deeper into statistics. Here is a selection from the Internet.\n",
    "\n",
    "Udacity's \"Intro to Statistics\": https://eu.udacity.com/course/intro-to-statistics--st101\n",
    "\n",
    "Udacity's \"Intro to Descriptive Statistics\": https://eu.udacity.com/course/intro-to-descriptive-statistics--ud827\n",
    "\n",
    "Udacity's \"Intro to Inferential Statistics\": https://eu.udacity.com/course/intro-to-inferential-statistics--ud201\n",
    "\n",
    "edX's \"Introduction to Probability - The Science of Uncertainty\" : https://www.edx.org/course/introduction-probability-science-mitx-6-041x-2\n",
    "\n",
    "Khan Academy's videos on statistics and probability: https://www.khanacademy.org/math/statistics-probability\n",
    "\n",
    "(Kaggle Kernel) Mathematics of Linear Regression by Nathan Lauga: https://www.kaggle.com/nathanlauga/mathematics-of-linear-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0cdc1afd3750d5ff5276f4dd7c2d2461f1a00d21"
   },
   "source": [
    "## Sources <a id=\"16\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "558357c8ce1f217b72b745f898ec51619a8379db"
   },
   "source": [
    "https://dataconomy.com/2015/02/introduction-to-bayes-theorem-with-python\n",
    "\n",
    "https://www.statisticshowto.datasciencecentral.com/discrete-variable/\n",
    "\n",
    "https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/binomial-theorem/binomial-distribution-formula/\n",
    "\n",
    "https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/descriptive-statistics/sample-variance/\n",
    "\n",
    "https://machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method\n",
    "\n",
    "https://machinelearningmastery.com/k-fold-cross-validation/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Poisson_distribution\n",
    "\n",
    "https://www.tutorialspoint.com/python/python_p_value.htm\n",
    "\n",
    "https://towardsdatascience.com/inferential-statistics-series-t-test-using-numpy-2718f8f9bf2f\n",
    "\n",
    "https://www.slideshare.net/dessybudiyanti/simple-linier-regression\n",
    "\n",
    "https://www.youtube.com/channel/UCgBncpylJ1kiVaPyP-PZauQ\n",
    "\n",
    "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
