{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c752e8f",
   "metadata": {},
   "source": [
    "## applying Q and sarsa on MountainCar  environment\n",
    "\n",
    "> ### states:\n",
    "2 continuous: position of the car and it's velocity\n",
    "\n",
    "> ### actions:\n",
    "Three possible actions: 0 means push left, 1 means do nothing (not sure why you’d ever do that), and 2 means push right.\n",
    "\n",
    "> ### rewards:\n",
    "They say you get “-1 for each time step until the goal position of 0.5 is reached”. So there’s no positive reward? Huh. They also say “there is no penalty for climbing the left hill, which upon reached acts as a wall”. I guess that means I can bounce my car off it.\n",
    "\n",
    "> ### episode terminates at:\n",
    "> 200 step for each episode\n",
    "\n",
    "> if the position of the car is at 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d523ea7b",
   "metadata": {},
   "source": [
    "# 1- import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a5b4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e07d8c",
   "metadata": {},
   "source": [
    "# 2- choose the environment and reset it to the initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a43637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.51846373,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aace53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## let's look at the states of our environment\n",
    "s=env.observation_space\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15e544",
   "metadata": {},
   "source": [
    "#### Box: means we have continuous number of states represented by two numbers:\n",
    "velocity of the car ranges -0.07 to 0.07\n",
    ",,position of the car ranges -1.2 to 0.6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b86121b",
   "metadata": {},
   "source": [
    "# 3- time to discritize our states to be able to build our Q_table \n",
    "##### note: we don't do this with discrete states type of environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1293728c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19 15]\n"
     ]
    }
   ],
   "source": [
    "num_states = (env.observation_space.high - env.observation_space.low)*\\\n",
    "                    np.array([10, 100])\n",
    "num_states = np.round(num_states, 0).astype(int) + 1\n",
    "    \n",
    "print(num_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ad0ab6",
   "metadata": {},
   "source": [
    "#### [19  15] means that we have 19 different position for the car, each has a velocity of the 15 we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75138729",
   "metadata": {},
   "source": [
    "# 4- build the Q_table and initialize it with 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2818096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Q table with 0\n",
    "Q = np.zeros(shape = (num_states[0], num_states[1],  env.action_space.n))\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927822c",
   "metadata": {},
   "source": [
    "# 5- now we can make a policy that agent can follow will playing the game\n",
    "\n",
    "choose_action(), what does it do?\n",
    "\n",
    "it's about choosing whether you want your agent to explore the environment or exploit it (explanation for both is in a notebook in the same repository), so:\n",
    "\n",
    "1- pick a random number\n",
    "\n",
    "2- if this number is less than epsilon, this means that epsilon is a big number so we will tend to explore and make random action\n",
    "\n",
    "3- if this number is larger than epsilon , this means that epsilon is a small number so we will tend to exploit and choose the action that the largest Q_value in the Q_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "820f9617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(env,epsilon, Q,state_adj):\n",
    "     if np.random.random() < epsilon:\n",
    "        return np.random.randint(0, env.action_space.n) \n",
    "     else:\n",
    "        return np.argmax(Q[state_adj[0], state_adj[1]])\n",
    "       \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbda07aa",
   "metadata": {},
   "source": [
    "# 6- now time to implemetent the Qlearning algorithm\n",
    "\n",
    "#### note :this algorithm can be implemented with much more simpler ways, all do the same thing, but the performance of the agent can differ from one to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01077cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(env, learning, discount, epsilon, min_eps, episodes):\n",
    "    #Q = np.random.uniform(low = -1, high = 1, size = (num_states[0], num_states[1], env.action_space.n))\n",
    "    Q = np.zeros(shape = (num_states[0], num_states[1],  env.action_space.n))\n",
    "    reduction = (epsilon - min_eps)/episodes\n",
    "    reward_list = []\n",
    "    ave_reward_list = []\n",
    "    for i in range(episodes):\n",
    "        # Initialize parameters\n",
    "        done = False\n",
    "        tot_reward, reward = 0,0\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Discretize state\n",
    "        state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "        state_adj = np.round(state_adj, 0).astype(int)\n",
    "        \n",
    "        while done != True:   \n",
    "           \n",
    "            # Determine next action - epsilon greedy strategy, you can call the function above\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(0, env.action_space.n)\n",
    "            else:\n",
    "                action = np.argmax(Q[state_adj[0], state_adj[1]]) \n",
    "                \n",
    "                \n",
    "            # Get next state and reward\n",
    "            state2, reward, done, info = env.step(action) \n",
    "            \n",
    "            # Discretize state2\n",
    "            state2_adj = (state2 - env.observation_space.low)*np.array([10, 100])\n",
    "            state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "            \n",
    "            #Allow for terminal states\n",
    "            if done and state2[0] >= 0.5:\n",
    "                Q[state_adj[0], state_adj[1], action] = reward\n",
    "                \n",
    "            # Adjust Q value for current state\n",
    "            else:\n",
    "                delta = learning*(reward + \n",
    "                                 discount*np.max(Q[state2_adj[0], state2_adj[1]]) -Q[state_adj[0], state_adj[1],action])\n",
    "                Q[state_adj[0], state_adj[1],action] += delta\n",
    "                                     \n",
    "            # Update variables\n",
    "            tot_reward += reward\n",
    "            state_adj = state2_adj\n",
    "        \n",
    "        # Decay epsilon, as we want to tend to exploit while discovering \n",
    "        if epsilon > min_eps:\n",
    "            epsilon -= reduction\n",
    "        \n",
    "        # Track rewards\n",
    "        reward_list.append(tot_reward)\n",
    "        #we want here to show the average of rewards over each 100 episode\n",
    "        if (i+1) % 100 == 0:\n",
    "            ave_reward = np.mean(reward_list)\n",
    "            ave_reward_list.append(ave_reward)\n",
    "            reward_list = []\n",
    "            \n",
    "        if (i+1) % 100 == 0:    \n",
    "            print('Episode {} Average Reward: {}'.format(i+1, ave_reward))\n",
    "            \n",
    "    env.close()\n",
    "    \n",
    "    return ave_reward_list\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a68dee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 Average Reward: -200.0\n",
      "Episode 200 Average Reward: -200.0\n",
      "Episode 300 Average Reward: -200.0\n",
      "Episode 400 Average Reward: -200.0\n",
      "Episode 500 Average Reward: -200.0\n",
      "Episode 600 Average Reward: -200.0\n",
      "Episode 700 Average Reward: -200.0\n",
      "Episode 800 Average Reward: -200.0\n",
      "Episode 900 Average Reward: -200.0\n",
      "Episode 1000 Average Reward: -200.0\n",
      "Episode 1100 Average Reward: -200.0\n",
      "Episode 1200 Average Reward: -200.0\n",
      "Episode 1300 Average Reward: -200.0\n",
      "Episode 1400 Average Reward: -200.0\n",
      "Episode 1500 Average Reward: -200.0\n",
      "Episode 1600 Average Reward: -200.0\n",
      "Episode 1700 Average Reward: -200.0\n",
      "Episode 1800 Average Reward: -200.0\n",
      "Episode 1900 Average Reward: -200.0\n",
      "Episode 2000 Average Reward: -200.0\n",
      "Episode 2100 Average Reward: -200.0\n",
      "Episode 2200 Average Reward: -200.0\n",
      "Episode 2300 Average Reward: -200.0\n",
      "Episode 2400 Average Reward: -200.0\n",
      "Episode 2500 Average Reward: -200.0\n",
      "Episode 2600 Average Reward: -200.0\n",
      "Episode 2700 Average Reward: -200.0\n",
      "Episode 2800 Average Reward: -200.0\n",
      "Episode 2900 Average Reward: -200.0\n",
      "Episode 3000 Average Reward: -199.34\n",
      "Episode 3100 Average Reward: -200.0\n",
      "Episode 3200 Average Reward: -198.78\n",
      "Episode 3300 Average Reward: -199.83\n",
      "Episode 3400 Average Reward: -199.58\n",
      "Episode 3500 Average Reward: -199.75\n",
      "Episode 3600 Average Reward: -199.39\n",
      "Episode 3700 Average Reward: -198.09\n",
      "Episode 3800 Average Reward: -198.99\n",
      "Episode 3900 Average Reward: -197.92\n",
      "Episode 4000 Average Reward: -198.43\n",
      "Episode 4100 Average Reward: -196.06\n",
      "Episode 4200 Average Reward: -199.51\n",
      "Episode 4300 Average Reward: -194.79\n",
      "Episode 4400 Average Reward: -192.91\n",
      "Episode 4500 Average Reward: -193.53\n",
      "Episode 4600 Average Reward: -173.42\n",
      "Episode 4700 Average Reward: -193.15\n",
      "Episode 4800 Average Reward: -191.03\n",
      "Episode 4900 Average Reward: -197.16\n",
      "Episode 5000 Average Reward: -199.94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Average Reward')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the performance of our agent\n",
    "rewards = QLearning(env, 0.2, 0.9, 0.8, 0, 5000)\n",
    "\n",
    "# Plot Rewards\n",
    "plt.plot(100*(np.arange(len(rewards)) + 1), rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1170e",
   "metadata": {},
   "source": [
    "# 7- let's see what sarsa would do! \n",
    "\n",
    "#### note: same as Q but, the Q function differs, instead of choosing the max Q_value of the current state, we will take the Q_value of the next action of the next state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f8aa8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, learning, discount, epsilon, min_eps, episodes):\n",
    "    #Q = np.random.uniform(low = -1, high = 1, size = (num_states[0], num_states[1], env.action_space.n))\n",
    "    Q = np.zeros(shape = (num_states[0], num_states[1],  env.action_space.n))\n",
    "    reduction = (epsilon - min_eps)/episodes\n",
    "    reward_list = []\n",
    "    ave_reward_list = []\n",
    "    for i in range(episodes):\n",
    "        # Initialize parameters\n",
    "        done = False\n",
    "        tot_reward, reward = 0,0\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Discretize state\n",
    "        state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "        state_adj = np.round(state_adj, 0).astype(int)\n",
    "        action=choose_action(env,epsilon,Q,state_adj)\n",
    "        while done != True:             \n",
    "            # Get next state and reward\n",
    "            state2, reward, done, info = env.step(action) \n",
    "            action2=choose_action(env,epsilon,Q,state_adj)\n",
    "            # Discretize state2\n",
    "            state2_adj = (state2 - env.observation_space.low)*np.array([10, 100])\n",
    "            state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "            \n",
    "            #Allow for terminal states, terminations is when position of the current state reaches 0.5\n",
    "            if done and state2[0] >= 0.5:\n",
    "                Q[state_adj[0], state_adj[1], action] = reward\n",
    "                \n",
    "            # Adjust Q value for current state\n",
    "            else:\n",
    "                delta = learning*(reward + discount* Q[state2_adj[0], state2_adj[1],action2]- Q[state_adj[0], state_adj[1],action])\n",
    "                Q[state_adj[0], state_adj[1],action] += delta\n",
    "                                     \n",
    "            # Update variables\n",
    "            tot_reward += reward\n",
    "            state_adj = state2_adj\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if epsilon > min_eps:\n",
    "            epsilon -= reduction\n",
    "        \n",
    "        # Track rewards\n",
    "        reward_list.append(tot_reward)\n",
    "        #we want here to show the average of rewards over each 100 episode\n",
    "        if (i+1) % 100 == 0:\n",
    "            ave_reward = np.mean(reward_list)\n",
    "            ave_reward_list.append(ave_reward)\n",
    "            reward_list = []\n",
    "            \n",
    "        if (i+1) % 100 == 0:    \n",
    "            print('Episode {} Average Reward: {}'.format(i+1, ave_reward))\n",
    "            \n",
    "    env.close()\n",
    "    \n",
    "    return ave_reward_list\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c219f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 Average Reward: -200.0\n",
      "Episode 200 Average Reward: -200.0\n",
      "Episode 300 Average Reward: -200.0\n",
      "Episode 400 Average Reward: -200.0\n",
      "Episode 500 Average Reward: -200.0\n",
      "Episode 600 Average Reward: -200.0\n",
      "Episode 700 Average Reward: -200.0\n",
      "Episode 800 Average Reward: -200.0\n",
      "Episode 900 Average Reward: -200.0\n",
      "Episode 1000 Average Reward: -200.0\n",
      "Episode 1100 Average Reward: -200.0\n",
      "Episode 1200 Average Reward: -200.0\n",
      "Episode 1300 Average Reward: -200.0\n",
      "Episode 1400 Average Reward: -200.0\n",
      "Episode 1500 Average Reward: -200.0\n",
      "Episode 1600 Average Reward: -200.0\n",
      "Episode 1700 Average Reward: -200.0\n",
      "Episode 1800 Average Reward: -200.0\n",
      "Episode 1900 Average Reward: -200.0\n",
      "Episode 2000 Average Reward: -200.0\n",
      "Episode 2100 Average Reward: -200.0\n",
      "Episode 2200 Average Reward: -200.0\n",
      "Episode 2300 Average Reward: -200.0\n",
      "Episode 2400 Average Reward: -200.0\n",
      "Episode 2500 Average Reward: -200.0\n",
      "Episode 2600 Average Reward: -200.0\n",
      "Episode 2700 Average Reward: -200.0\n",
      "Episode 2800 Average Reward: -200.0\n",
      "Episode 2900 Average Reward: -200.0\n",
      "Episode 3000 Average Reward: -200.0\n",
      "Episode 3100 Average Reward: -200.0\n",
      "Episode 3200 Average Reward: -200.0\n",
      "Episode 3300 Average Reward: -200.0\n",
      "Episode 3400 Average Reward: -200.0\n",
      "Episode 3500 Average Reward: -200.0\n",
      "Episode 3600 Average Reward: -200.0\n",
      "Episode 3700 Average Reward: -200.0\n",
      "Episode 3800 Average Reward: -200.0\n",
      "Episode 3900 Average Reward: -200.0\n",
      "Episode 4000 Average Reward: -200.0\n",
      "Episode 4100 Average Reward: -200.0\n",
      "Episode 4200 Average Reward: -200.0\n",
      "Episode 4300 Average Reward: -200.0\n",
      "Episode 4400 Average Reward: -200.0\n",
      "Episode 4500 Average Reward: -200.0\n",
      "Episode 4600 Average Reward: -200.0\n",
      "Episode 4700 Average Reward: -200.0\n",
      "Episode 4800 Average Reward: -200.0\n",
      "Episode 4900 Average Reward: -200.0\n",
      "Episode 5000 Average Reward: -200.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Average Reward')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the performance\n",
    "rewards = sarsa(env, 0.2, 0.9, 0.8, 0, 5000)\n",
    "\n",
    "# Plot Rewards\n",
    "plt.plot(100*(np.arange(len(rewards)) + 1), rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343773a5",
   "metadata": {},
   "source": [
    "> ## as we can see here, the performance of the agent while Q_learning is better than sarsa (in this environment !)\n",
    "\n",
    "> ## try different values for parameters and see the results\n",
    "\n",
    "> ## this algorithm can be implemented in much more simpler way, but here seemed complicated because we have continuous range  type of states \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df28227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
